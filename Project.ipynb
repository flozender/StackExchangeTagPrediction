{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Word2Vec\n",
    "import numpy \n",
    "\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font size=5 color=red>Investigate the original dataset (obviously, it cannot be used). Take a look at https://stackoverflow.com/questions/13793529/r-error-invalid-type-list-for-variable to see how useless the Body column information could be!\n",
    "\n",
    "The point here is that the body information consists mostly of codes and some weird patterns that are not useful for our purpose. The most important information here is the connection between the title of the questions and tags. So, I removed the Body column from the dataset.</Font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62819203\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                  Id|               Title|                Body|                Tags|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                   1|How to check if a...|<p>I'd like to ch...|php image-process...|\n",
      "|                   2|How can I prevent...|<p>In my favorite...|             firefox|\n",
      "|                   3|R Error Invalid t...|\"<p>I am import m...|                null|\n",
      "|      expert_trai...|                null|                null|                null|\n",
      "|      expert_data...|                null|                null|                null|\n",
      "|      rf_model = ...| data=expert_data...|     importance=TRUE|      do.trace=100);|\n",
      "|                   }|                null|                null|                null|\n",
      "|       </code></pre>|                null|                null|                null|\n",
      "|<p>Structure of t...|                null|                null|                null|\n",
      "|<pre><code>[56x12...|                null|                null|                null|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = init_spark()\n",
    "\n",
    "    filename1 = \"./Train.csv\"\n",
    "    df2 = spark.read.option(\"multiLine\", 'true').option(\"escape\",\"\\'\").csv(filename1, header=True)\n",
    "    print(df2.count())\n",
    "    print(df2.show(10))    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font size=5 color=red>For removing the Body column, I read all the dataset once using Pandas library. After that, I removed the column and got an export to have a concrete file as our dataset. This part has been ommited from the notebook.</Font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 17:38:06 WARN Utils: Your hostname, Ashrafs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.239 instead (on interface en0)\n",
      "22/03/28 17:38:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/03/28 17:38:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6017243"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "filename = \"./TrainWithoutBody.csv\"\n",
    "df1 = spark.read.option(\"multiLine\", 'true').option(\"escape\",\"\\'\").csv(filename, header=True)\n",
    "df1 = df1.drop(\"_c0\")\n",
    "df1 = df1.dropna()\n",
    "\n",
    "rddTags = df1.select(\"Tags\").rdd\n",
    "\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| Id|               Title|                Tags|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|How to check if a...|php image-process...|\n",
      "|  2|How can I prevent...|             firefox|\n",
      "|  3|R Error Invalid t...|r matlab machine-...|\n",
      "|  4|How do I replace ...|     c# url encoding|\n",
      "|  5|How to modify who...|php api file-get-...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font size = 5, color=green>Finding the 100 most used tags (one DT per each most used tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \"\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "splittedTags = rddTags.filter(lambda r: r[0] != None).flatMap(lambda r: r[0].split(\" \")).map(lambda r: (r, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "splittedTags = splittedTags.sortBy(lambda r: r[1], False) #Sorted with number of usage (you can collect and see)\n",
    "\n",
    "splittedTagsSorted = splittedTags.map(lambda r: r[0]) #Delete this line if you want to see number of times they have been used.\n",
    "\n",
    "# df10 = anSorted.toDF()\n",
    "\n",
    "\n",
    "mostUsedTags = splittedTagsSorted.collect()[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c#',\n",
       " 'java',\n",
       " 'php',\n",
       " 'javascript',\n",
       " 'android',\n",
       " 'jquery',\n",
       " 'c++',\n",
       " 'python',\n",
       " 'iphone',\n",
       " 'asp.net',\n",
       " 'mysql',\n",
       " 'html',\n",
       " '.net',\n",
       " 'ios',\n",
       " 'objective-c',\n",
       " 'sql',\n",
       " 'css',\n",
       " 'linux',\n",
       " 'ruby-on-rails',\n",
       " 'windows',\n",
       " 'c',\n",
       " 'sql-server',\n",
       " 'ruby',\n",
       " 'wpf',\n",
       " 'xml',\n",
       " 'ajax',\n",
       " 'database',\n",
       " 'regex',\n",
       " 'windows-7',\n",
       " 'asp.net-mvc',\n",
       " 'xcode',\n",
       " 'django',\n",
       " 'osx',\n",
       " 'arrays',\n",
       " 'vb.net',\n",
       " 'eclipse',\n",
       " 'json',\n",
       " 'facebook',\n",
       " 'ruby-on-rails-3',\n",
       " 'ubuntu',\n",
       " 'performance',\n",
       " 'networking',\n",
       " 'string',\n",
       " 'multithreading',\n",
       " 'winforms',\n",
       " 'security',\n",
       " 'asp.net-mvc-3',\n",
       " 'visual-studio-2010',\n",
       " 'bash',\n",
       " 'homework',\n",
       " 'image',\n",
       " 'wcf',\n",
       " 'html5',\n",
       " 'wordpress',\n",
       " 'web-services',\n",
       " 'visual-studio',\n",
       " 'forms',\n",
       " 'algorithm',\n",
       " 'sql-server-2008',\n",
       " 'linq',\n",
       " 'oracle',\n",
       " 'git',\n",
       " 'query',\n",
       " 'perl',\n",
       " 'apache2',\n",
       " 'flash',\n",
       " 'actionscript-3',\n",
       " 'ipad',\n",
       " 'spring',\n",
       " 'apache',\n",
       " 'silverlight',\n",
       " 'email',\n",
       " 'r',\n",
       " 'cocoa-touch',\n",
       " 'cocoa',\n",
       " 'swing',\n",
       " 'hibernate',\n",
       " 'excel',\n",
       " 'entity-framework',\n",
       " 'file',\n",
       " 'shell',\n",
       " 'flex',\n",
       " 'api',\n",
       " 'list',\n",
       " 'internet-explorer',\n",
       " 'firefox',\n",
       " 'jquery-ui',\n",
       " 'delphi',\n",
       " '.htaccess',\n",
       " 'sqlite',\n",
       " 'qt',\n",
       " 'tsql',\n",
       " 'google-chrome',\n",
       " 'node.js',\n",
       " 'unix',\n",
       " 'windows-xp',\n",
       " 'http',\n",
       " 'svn',\n",
       " 'unit-testing',\n",
       " 'oop']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostUsedTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6017243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rddTags.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Tags='php image-processing file-upload upload mime-types'),\n",
       " Row(Tags='firefox'),\n",
       " Row(Tags='r matlab machine-learning'),\n",
       " Row(Tags='c# url encoding'),\n",
       " Row(Tags='php api file-get-contents'),\n",
       " Row(Tags='proxy active-directory jmeter'),\n",
       " Row(Tags='core-plot'),\n",
       " Row(Tags='c# asp.net windows-phone-7'),\n",
       " Row(Tags='.net javascript code-generation'),\n",
       " Row(Tags='sql variables parameters procedure calls')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTags.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font size=5.5, color=\"purpule\">Here, I have cleaned the Tags column to only contain the most used tags. For example, I ommited the \"upload\" tag from first group of tags for the first question, because it's not a most used tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNoneWithString(x):\n",
    "    if (x == None): return \"None\"\n",
    "    else : return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['php'],\n",
       " ['firefox'],\n",
       " ['r'],\n",
       " ['c#'],\n",
       " ['php', 'api'],\n",
       " [],\n",
       " [],\n",
       " ['c#', 'asp.net'],\n",
       " ['.net', 'javascript'],\n",
       " ['sql']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrr = rddTags.map(lambda r: r[0]).map(replaceNoneWithString).map(lambda r: r.split(\" \")).map(lambda r: [ped for ped in r if ped in mostUsedTags])\n",
    "cleanedTags = rrr.take(10)\n",
    "cleanedTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject titles to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Title\", outputCol=\"transformed_tfidf\")\n",
    "wordsData = tokenizer.transform(df1)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"transformed_tfidf\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.take(1)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                tags|            features|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|php image-process...|(20,[1,3,4,8,9,10...|\n",
      "|  2|             firefox|(20,[1,2,3,7,12,1...|\n",
      "|  3|r matlab machine-...|(20,[2,4,5,6,7,17...|\n",
      "|  4|     c# url encoding|(20,[3,6,7,10,13,...|\n",
      "|  5|php api file-get-...|(20,[3,5,8,13,15,...|\n",
      "|  6|proxy active-dire...|(20,[0,3,4,7,13,1...|\n",
      "|  7|           core-plot|(20,[3,6,7,8,10,1...|\n",
      "|  8|c# asp.net window...|(20,[0,3,7,8,10,1...|\n",
      "|  9|.net javascript c...|(20,[0,1,3,4],[1....|\n",
      "| 10|sql variables par...|(20,[0,3,6,12,16,...|\n",
      "| 11|.net obfuscation ...|(20,[0,3,6,8,11,1...|\n",
      "| 12|algorithm languag...|(20,[0,1,5,19],[1...|\n",
      "| 13|postfix migration...|(20,[1,3,8,18,19]...|\n",
      "| 14|documentation lat...|(20,[10,13,16,17,...|\n",
      "| 15|           windows-7|(20,[1,2,12,13,15...|\n",
      "| 16|php url-routing c...|(20,[2,4,10,18],[...|\n",
      "| 17|   r temporary-files|(20,[3,5,8,13,15,...|\n",
      "| 18|         wpf binding|(20,[0,8,12,13,15...|\n",
      "| 19|javascript code-g...|(20,[6,11,13,14,1...|\n",
      "| 20|php xml hash mult...|(20,[1,2,7,8,10,1...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Id='1', Title='How to check if an uploaded file is an image without mime type?', Tags='php image-processing file-upload upload mime-types', transformed_tfidf=['how', 'to', 'check', 'if', 'an', 'uploaded', 'file', 'is', 'an', 'image', 'without', 'mime', 'type?'], rawFeatures=SparseVector(20, {1: 1.0, 3: 3.0, 4: 1.0, 8: 2.0, 9: 1.0, 10: 2.0, 12: 2.0, 16: 1.0}), features=SparseVector(20, {1: 1.1357, 3: 1.8792, 4: 1.1753, 8: 1.4617, 9: 1.1974, 10: 2.0765, 12: 2.0979, 16: 0.9553}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.select(\"id\", \"tags\", \"features\").show()\n",
    "rescaledData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject titles to Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 17:42:43 WARN MemoryStore: Not enough space to cache rdd_60_0 in memory! (computed 248.3 MiB so far)\n",
      "22/03/28 17:42:43 WARN BlockManager: Block rdd_60_0 could not be removed as it was not found on disk or in memory\n",
      "22/03/28 17:42:43 WARN BlockManager: Putting block rdd_60_0 failed\n",
      "22/03/28 17:42:44 WARN MemoryStore: Not enough space to cache broadcast_25 in memory! (computed 60.2 MiB so far)\n",
      "22/03/28 17:42:45 WARN MemoryStore: Not enough space to cache broadcast_22 in memory! (computed 68.8 MiB so far)\n",
      "22/03/28 17:42:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/03/28 17:42:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Title\", outputCol=\"tokenized_text\")\n",
    "tokenized_df = tokenizer.transform(df1)\n",
    "\n",
    "word2Vec = Word2Vec(inputCol=\"tokenized_text\", outputCol=\"features\", vectorSize=100)\n",
    "w2v_model = word2Vec.fit(tokenized_df)\n",
    "w2v_data = w2v_model.transform(tokenized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df1.sample(False, 0.0083, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer_sampled = Tokenizer(inputCol=\"Title\", outputCol=\"tokenized_text\")\n",
    "tokenized_df_sampled = tokenizer.transform(sampled_df)\n",
    "\n",
    "word2Vec_sampled = Word2Vec(inputCol=\"tokenized_text\", outputCol=\"features\", vectorSize=100)\n",
    "w2v_model_sampled = word2Vec_sampled.fit(tokenized_df_sampled)\n",
    "w2v_data_sampled = w2v_model_sampled.transform(tokenized_df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='256', Title='How to figure out all colors in a gradient?', Tags='javascript html html5 colors mobile-safari', tokenized_text=['how', 'to', 'figure', 'out', 'all', 'colors', 'in', 'a', 'gradient?'], features=DenseVector([0.0453, 0.011, 0.0179, 0.0552, 0.0396, -0.072, -0.0676, 0.0642, -0.0216, 0.0092, 0.003, -0.0752, -0.0464, -0.0993, 0.0792, -0.0697, 0.0541, 0.0066, 0.0379, -0.0012, -0.1039, 0.0168, 0.0468, 0.0678, -0.0138, 0.0469, -0.043, 0.0017, -0.0636, -0.0329, -0.0881, -0.0813, 0.0638, -0.0253, 0.019, -0.0105, -0.0076, -0.1132, -0.0675, 0.0242, 0.0062, -0.0018, 0.0677, 0.1666, -0.0445, 0.0517, -0.0105, 0.0297, -0.0228, 0.0368, 0.0895, -0.0697, -0.0205, -0.0536, -0.0087, 0.0111, -0.025, 0.0166, 0.0306, -0.0238, -0.0804, -0.0428, -0.0046, 0.0559, 0.0364, 0.0476, -0.007, -0.0841, 0.0906, 0.0261, 0.0482, 0.0353, 0.0064, -0.1306, -0.0393, -0.0606, 0.0143, 0.0826, -0.017, 0.0578, -0.1151, 0.0087, 0.1127, -0.0206, -0.0386, -0.0009, -0.0453, 0.0112, -0.0217, 0.0935, -0.0431, 0.0411, -0.0746, -0.0279, -0.0467, -0.0626, -0.046, 0.0076, 0.0156, 0.0351]))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_data_sampled.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Tag_Array=['javascript', 'html', 'html5', 'colors', 'mobile-safari'])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets = [lambda r: r.split(' '), w2v_data_sampled.select(\"Tags\").collect()]\n",
    "target_tags = w2v_data_sampled.select(split(col(\"Tags\"),\" \").alias(\"Tag_Array\"))\n",
    "target_tags.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 19:05:47 ERROR Executor: Exception in task 0.0 in stage 43.0 (TID 43)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/03/28 19:05:47 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 43) (10.0.0.239 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"<frozen zipimport>\", line 259, in load_module\n",
      "  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/03/28 19:05:47 ERROR TaskSetManager: Task 0 in stage 43.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 43) (10.0.0.239 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/ashrafkhalil/Documents/SOEN-6111/Project.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ashrafkhalil/Documents/SOEN-6111/Project.ipynb#ch0000029?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (r\u001b[39m.\u001b[39mid, r\u001b[39m.\u001b[39mTitle, r\u001b[39m.\u001b[39mfeatures, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ashrafkhalil/Documents/SOEN-6111/Project.ipynb#ch0000029?line=10'>11</a>\u001b[0m w2v_with_targets \u001b[39m=\u001b[39m w2v_data_sampled\u001b[39m.\u001b[39mrdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m r: my_function(r))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ashrafkhalil/Documents/SOEN-6111/Project.ipynb#ch0000029?line=11'>12</a>\u001b[0m w2v_with_targets\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py:1566\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py?line=1562'>1563</a>\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py?line=1564'>1565</a>\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py?line=1565'>1566</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py?line=1567'>1568</a>\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py?line=1568'>1569</a>\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py:1233\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1228'>1229</a>\u001b[0m \u001b[39m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1229'>1230</a>\u001b[0m \u001b[39m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1230'>1231</a>\u001b[0m \u001b[39m# SparkContext#runJob.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1231'>1232</a>\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1232'>1233</a>\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/context.py?line=1233'>1234</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1305'>1306</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1308'>1309</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1309'>1310</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/lab1/lib/python3.8/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 43) (10.0.0.239 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 893, in _parse_datatype_json_string\n    return _parse_datatype_json_value(json.loads(json_string))\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 910, in _parse_datatype_json_value\n    return _all_complex_types[tpe].fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in fromJson\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 596, in <listcomp>\n    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 441, in fromJson\n    _parse_datatype_json_value(json[\"type\"]),\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n    return UserDefinedType.fromJson(json_value)\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 755, in fromJson\n    m = __import__(pyModule, globals(), locals(), [pyClass])\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n  File \"<frozen zipimport>\", line 259, in load_module\n  File \"/opt/anaconda3/envs/lab1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def my_function(r):\n",
    "    r = r.Tag_Array\n",
    "    target = []\n",
    "    for t in mostUsedTags:\n",
    "        if t in r:\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n",
    "    return (r.id, r.Title, r.features, target)\n",
    "\n",
    "w2v_with_targets = w2v_data_sampled.rdd.map(lambda r: my_function(r))\n",
    "w2v_with_targets.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
